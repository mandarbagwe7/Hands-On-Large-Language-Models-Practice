{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9U4eH6fC5CpxHwDsQqdE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandarbagwe7/Hands-On-Large-Language-Models-Practice/blob/main/chapter_02/Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XWDijYDWiZFv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model and Tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "yjQPnnCLiysB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. <|assistant|>\""
      ],
      "metadata": {
        "id": "ETEeAK-ri7cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "input_ids"
      ],
      "metadata": {
        "id": "2-PsNS5ZjZlp",
        "outputId": "9d21d161-ba52-4655-99f2-cf0d12e11a4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
              "           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n",
              "           372,  9559, 29889, 29871, 32001]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    max_new_tokens = 30,\n",
        "    use_cache=False\n",
        ")\n",
        "\n",
        "generation_output[0]"
      ],
      "metadata": {
        "id": "xkn491K7jrl8",
        "outputId": "64fa9a7c-5618-4903-cf22-938d5ee449e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
              "          293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n",
              "          372,  9559, 29889, 29871, 32001,  3323,   622, 29901,   317,  3742,\n",
              "          406,  6225, 11763,   363,   278, 19906,   292,   341,   728,   481,\n",
              "           13,    13,    13, 29928,   799, 19235, 29892,    13,    13,    13,\n",
              "        29902,  4966,   445,  2643, 14061], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the output\n",
        "print(tokenizer.decode(generation_output[0]))"
      ],
      "metadata": {
        "id": "AsrXGrL2kGBO",
        "outputId": "7b9f0463-11b0-4e2e-a864-2ab5e5fde549",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. <|assistant|> Subject: Sincere Apologies for the Gardening Mishap\n",
            "\n",
            "\n",
            "Dear Sarah,\n",
            "\n",
            "\n",
            "I hope this message finds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "\n",
        "English and CAPITALIZATION\n",
        "\n",
        "ðŸŽµé¸Ÿ\n",
        "\n",
        "show_tokens False None elif == >= else: two tabs:\" \" Three tabs: \"   \"\n",
        "\n",
        "12.0*50=600\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sfVIBKSDksKg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors_list = [\n",
        "'102;194;165', '252;141;98', '141;160;203',\n",
        "'231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' + tokenizer.decode(t) + '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ],
      "metadata": {
        "id": "mNmKZ-fGCInp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"google-bert/bert-base-uncased\")"
      ],
      "metadata": {
        "id": "B6PmWI1oCgJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"google-bert/bert-base-cased\")"
      ],
      "metadata": {
        "id": "7L9RAgkFCkpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"openai-community/gpt2\")"
      ],
      "metadata": {
        "id": "NYW-8sXNCq84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNkAQlTQFIZh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}