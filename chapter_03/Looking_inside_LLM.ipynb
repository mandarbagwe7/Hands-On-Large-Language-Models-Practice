{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqhvTRbUv+KRaQwkWbFBrt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandarbagwe7/Hands-On-Large-Language-Models-Practice/blob/main/chapter_03/Looking_inside_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.5.0"
      ],
      "metadata": {
        "id": "q1vz20QTG_sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zHkoYeQyGFx-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "Ug7LtPB7Gddc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "SRXLNpXeGhil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False,\n",
        ")"
      ],
      "metadata": {
        "id": "UN6A8C79HGQZ",
        "outputId": "da638c9e-5e9e-4e51-d170-05c395e8167c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "output = generator(prompt, use_cache=False)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "NZ4zlpPQHTKu",
        "outputId": "042df45e-2cf1-4655-826f-f2418d470c3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct.f39ac1d28e925b323eae81227eaba4464caced4e.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mention the steps you're taking to prevent it in the future.\n",
            "\n",
            "Email:\n",
            "\n",
            "Subject: Sincere Apologies for the Gardening Mishap\n",
            "\n",
            "Dear Sarah,\n",
            "\n",
            "I hope this email finds you well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The capital of France is\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "# Get the output of the model before the lm_head\n",
        "model_output = model.model(input_ids, use_cache=False)\n",
        "\n",
        "# Get the output of the lm_head\n",
        "lm_head_output = model.lm_head(model_output[0])"
      ],
      "metadata": {
        "id": "5e4niLFTHc-h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output[0].shape"
      ],
      "metadata": {
        "id": "YywS9xSULp61",
        "outputId": "2ec7f141-75c5-431a-e466-c68a66adb2d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output.shape"
      ],
      "metadata": {
        "id": "c674u0a3H0RO",
        "outputId": "fe8fa45c-9d34-4a5d-c098-5f6eb41b1c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32064])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output"
      ],
      "metadata": {
        "id": "QpGp_RTqH5ML",
        "outputId": "700c3829-eb89-4620-c776-c900fa83a1c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[24.7500, 24.8750, 22.7500,  ..., 19.0000, 19.0000, 19.0000],\n",
              "         [31.1250, 31.5000, 26.0000,  ..., 26.0000, 26.0000, 26.0000],\n",
              "         [31.5000, 28.8750, 31.1250,  ..., 26.2500, 26.2500, 26.2500],\n",
              "         [33.0000, 31.8750, 36.0000,  ..., 27.8750, 27.8750, 27.8750],\n",
              "         [27.8750, 29.5000, 28.1250,  ..., 20.5000, 20.5000, 20.5000]]],\n",
              "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output[0, -1]"
      ],
      "metadata": {
        "id": "dFJ_oojSH6fH",
        "outputId": "190a7dba-239c-4711-9718-a9440c20e2e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([27.8750, 29.5000, 28.1250,  ..., 20.5000, 20.5000, 20.5000],\n",
              "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = lm_head_output[0, -1].argmax(-1)\n",
        "tokenizer.decode(token_id)"
      ],
      "metadata": {
        "id": "cWssJQZKIfzr",
        "outputId": "76fef9a3-3458-4df5-9d68-0ee7d461efc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Paris'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xtL9kxBmI6Rh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}